{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AWS BlazingText\n",
    "\n",
    "The bag of words was not giving me any satisfactory results, whether using single words or 2-grams. So, searching for another method of text classification, I stumbled upon AWS' BlazingText algorithm. From BlazingText documentation:\n",
    "\n",
    "_The Amazon SageMaker BlazingText algorithm provides highly optimized implementations of the Word2vec and text classification algorithms. The Word2vec algorithm is useful for many downstream natural language processing (NLP) tasks, such as sentiment analysis, named entity recognition, machine translation, etc._\n",
    "\n",
    "## Set up environment\n",
    "\n",
    "### Download dataset manually\n",
    "\n",
    "Let's start downloading the train dataset from Kaggle's website:\n",
    "\n",
    "https://www.kaggle.com/c/nlp-getting-started/data\n",
    "\n",
    "I downloaded the dataset manually and saved if in the 'input' folder.\n",
    "\n",
    "The dataset could be downloaded programmatically using Kaggle API, but it requires authentication and it would be unsafe on a notebook that is meant to be shared."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read dataset from CSV\n",
    "\n",
    "Read the dataset from the CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   keyword location                                               text  target\n",
       "id                                                                            \n",
       "1      NaN      NaN  Our Deeds are the Reason of this #earthquake M...       1\n",
       "4      NaN      NaN             Forest fire near La Ronge Sask. Canada       1\n",
       "5      NaN      NaN  All residents asked to 'shelter in place' are ...       1\n",
       "6      NaN      NaN  13,000 people receive #wildfires evacuation or...       1\n",
       "7      NaN      NaN  Just got sent this photo from Ruby #Alaska as ...       1"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Read train dataset\n",
    "train_df = pd.read_csv('input/train.csv', encoding = 'ISO-8859-1', index_col='id')\n",
    "\n",
    "# Read test dataset\n",
    "test_df = pd.read_csv('input/test.csv', encoding = 'ISO-8859-1', index_col='id')\n",
    "\n",
    "# Check\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text tokenization\n",
    "\n",
    "Tweets are tokenized in a multi-step process:\n",
    "1. **Remove hyperlinks:** Replace hyperlinks with the keyword “islink”.\n",
    "1. **Numbers:** Replace all numbers with the keyword “isnumber”.\n",
    "1. **Punctuation:** Remove any non-alphanumeric character.\n",
    "1. **Case:** Make all tweets lowercase.\n",
    "1. **Split** text into different words.\n",
    "1. **Remove stopwords**.\n",
    "1. **Stem words**.\n",
    "1. **Label tokenized strings** as per BlazingText's requirements.\n",
    "\n",
    "I use the _Natural Language Toolkit_ and _Regular Expressions_ to tokenize tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import *\n",
    "import re\n",
    "\n",
    "def tweet_to_words(text):\n",
    "    nltk.download('stopwords', quiet=True)\n",
    "    stemmer = PorterStemmer()\n",
    "\n",
    "    text = re.sub(r'http\\S+', ' islink ', text) # replace hyperlinks with keyword\n",
    "    text = re.sub(r'(?!,$)[\\d,.]+', ' isnumber ', text) # replace numbers with keyword\n",
    "    text = re.sub(r'[^a-zA-Z0-0]', ' ', text.lower()) # Remove non-alphanumeric characters and convert to lower case\n",
    "    words = text.split() # Split string into words\n",
    "    words = [w for w in words if w not in stopwords.words(\"english\")] # Remove stopwords\n",
    "    words = [PorterStemmer().stem(w) for w in words] # stem\n",
    "            \n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['__label__1', 'deed', 'reason', 'earthquak', 'may', 'allah', 'forgiv', 'us'], ['__label__1', 'forest', 'fire', 'near', 'la', 'rong', 'sask', 'isnumb', 'canada'], ['__label__1', 'resid', 'ask', 'shelter', 'place', 'notifi', 'offic', 'isnumb', 'evacu', 'shelter', 'place', 'order', 'expect'], ['__label__1', 'isnumb', 'peopl', 'receiv', 'wildfir', 'evacu', 'order', 'california'], ['__label__1', 'got', 'sent', 'photo', 'rubi', 'alaska', 'smoke', 'wildfir', 'pour', 'school']]\n"
     ]
    }
   ],
   "source": [
    "# Train dataset\n",
    "train_set = []\n",
    "for index, row in train_df.iterrows():\n",
    "    tokenized_text = tweet_to_words(row['text'])\n",
    "    tokenized_text.insert(0, '__label__' + str(row['target'])) # blazingtext algorithm expects labels to be inserted and appended with __label__\n",
    "    train_set.append(tokenized_text)\n",
    "\n",
    "# Test dataset\n",
    "#test_set = []\n",
    "#for index, row in test_df.iterrows():\n",
    "#    test_set.append(tweet_to_words(row['text']))\n",
    "\n",
    "print(train_set[:5]) # Check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save data to train the model\n",
    "\n",
    "### Save train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import pickle\n",
    "\n",
    "data_dir = 'data' # The folder we will use for storing data\n",
    "if not os.path.exists(data_dir): # Make sure that the folder exists\n",
    "    os.makedirs(data_dir)\n",
    "\n",
    "with open(data_dir + '/train.txt', 'w') as csvoutfile:\n",
    "    csv_writer = csv.writer(csvoutfile, delimiter=' ', lineterminator='\\n')\n",
    "    csv_writer.writerows(train_set)\n",
    "#with open(data_dir + '/test.txt', 'w') as csvoutfile:\n",
    "#    csv_writer = csv.writer(csvoutfile, delimiter=' ', lineterminator='\\n')\n",
    "#    csv_writer.writerows(test_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload data to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arn:aws:iam::496791827306:role/service-role/AmazonSageMaker-ExecutionRole-20191201T135241\n",
      "sagemaker-eu-west-1-496791827306\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "import json\n",
    "import boto3\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "\n",
    "role = get_execution_role()\n",
    "print(role) # This is the role that SageMaker would use to leverage AWS resources (S3, CloudWatch) on your behalf\n",
    "\n",
    "bucket = sess.default_bucket() # Replace with your own bucket name if needed\n",
    "print(bucket)\n",
    "prefix = 'blazingtext/supervised' #Replace with the prefix under which you want to store the data if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 22.7 ms, sys: 4.16 ms, total: 26.9 ms\n",
      "Wall time: 117 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "train_channel = prefix + '/train'\n",
    "#validation_channel = prefix + '/validation'\n",
    "\n",
    "sess.upload_data(path=data_dir + '/train.txt', bucket=bucket, key_prefix=train_channel)\n",
    "#sess.upload_data(path='tweets.validation', bucket=bucket, key_prefix=validation_channel)\n",
    "\n",
    "s3_train_data = 's3://{}/{}'.format(bucket, train_channel)\n",
    "#s3_validation_data = 's3://{}/{}'.format(bucket, validation_channel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_output_location = 's3://{}/{}/output'.format(bucket, prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "region_name = boto3.Session().region_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training model\n",
    "\n",
    "### Set up training instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using SageMaker BlazingText container: 685385470294.dkr.ecr.eu-west-1.amazonaws.com/blazingtext:latest (eu-west-1)\n"
     ]
    }
   ],
   "source": [
    "container = sagemaker.amazon.amazon_estimator.get_image_uri(region_name, \"blazingtext\", \"latest\")\n",
    "print('Using SageMaker BlazingText container: {} ({})'.format(container, region_name))\n",
    "\n",
    "train_data = sagemaker.session.s3_input(s3_train_data, distribution='FullyReplicated', content_type='text/plain', s3_data_type='S3Prefix')\n",
    "data_channels = {'train': train_data}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "bt_model = sagemaker.estimator.Estimator(container,\n",
    "                                         role, \n",
    "                                         train_instance_count=1, \n",
    "                                         train_instance_type='ml.m4.xlarge',\n",
    "                                         train_volume_size = 30,\n",
    "                                         train_max_run = 360000,\n",
    "                                         input_mode= 'File',\n",
    "                                         output_path=s3_output_location,\n",
    "                                         sagemaker_session=sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "bt_model.set_hyperparameters(mode=\"supervised\",\n",
    "                            evaluation=False,\n",
    "                            epochs=10,\n",
    "                            min_count=2,\n",
    "                            learning_rate=0.05,\n",
    "                            vector_dim=10,\n",
    "                            early_stopping=False,\n",
    "                            patience=4,\n",
    "                            min_epochs=5,\n",
    "                            word_ngrams=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-01-20 18:05:02 Starting - Starting the training job...\n",
      "2020-01-20 18:05:04 Starting - Launching requested ML instances......\n",
      "2020-01-20 18:06:30 Starting - Preparing the instances for training......\n",
      "2020-01-20 18:07:25 Downloading - Downloading input data...\n",
      "2020-01-20 18:07:54 Training - Downloading the training image..\u001b[34mArguments: train\u001b[0m\n",
      "\u001b[34m[01/20/2020 18:08:14 WARNING 140231241815872] Loggers have already been setup.\u001b[0m\n",
      "\u001b[34m[01/20/2020 18:08:14 WARNING 140231241815872] Loggers have already been setup.\u001b[0m\n",
      "\u001b[34m[01/20/2020 18:08:14 INFO 140231241815872] nvidia-smi took: 0.025181055069 secs to identify 0 gpus\u001b[0m\n",
      "\u001b[34m[01/20/2020 18:08:14 INFO 140231241815872] Running single machine CPU BlazingText training using supervised mode.\u001b[0m\n",
      "\u001b[34m[01/20/2020 18:08:14 INFO 140231241815872] 2 files found in train channel. Using /opt/ml/input/data/train/train.txt for training...\u001b[0m\n",
      "\u001b[34m[01/20/2020 18:08:14 INFO 140231241815872] Processing /opt/ml/input/data/train/train.txt . File size: 0 MB\u001b[0m\n",
      "\u001b[34mRead 0M words\u001b[0m\n",
      "\u001b[34mNumber of words:  5294\u001b[0m\n",
      "\u001b[34m##### Alpha: 0.0000  Progress: 99.94%  Million Words/sec: 9.71 #####\u001b[0m\n",
      "\u001b[34m##### Alpha: 0.0000  Progress: 100.00%  Million Words/sec: 4.87 #####\n",
      "\u001b[0m\n",
      "\u001b[34mTraining finished.\u001b[0m\n",
      "\u001b[34mAverage throughput in Million words/sec: 4.87\u001b[0m\n",
      "\u001b[34mTotal training time in seconds: 0.20\n",
      "\u001b[0m\n",
      "\u001b[34m#train_accuracy: 0.9065\u001b[0m\n",
      "\u001b[34mNumber of train examples: 7613\u001b[0m\n",
      "\n",
      "2020-01-20 18:08:30 Uploading - Uploading generated training model\n",
      "2020-01-20 18:08:30 Completed - Training job completed\n",
      "Training seconds: 65\n",
      "Billable seconds: 65\n"
     ]
    }
   ],
   "source": [
    "bt_model.fit(inputs=data_channels, logs=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------!"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'blazingtext-2020-01-20-18-05-02-792'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Deploy estimator\n",
    "\n",
    "# Remember to shut down the endpoint when we are done!!!!\n",
    "\n",
    "text_classifier = bt_model.deploy(initial_instance_count = 1,instance_type = 'ml.m4.xlarge')\n",
    "text_classifier.endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing\n",
    "\n",
    "### Prepare test data & predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "for index, row in test_df.iterrows():\n",
    "    tokenized_sentence = ' '.join(tweet_to_words(row['text'])) # Tokenize text\n",
    "    payload = {\"instances\" : [tokenized_sentence]} # Format data for prediction\n",
    "    response = text_classifier.predict(json.dumps(payload)) # Predict\n",
    "    prediction = json.loads(response) # Load response\n",
    "    if prediction[0]['label'][0] == '__label__1': # Save value accordingly\n",
    "        predictions.append(np.int(1))\n",
    "    else:\n",
    "        predictions.append(np.int(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    target\n",
       "id        \n",
       "0        1\n",
       "2        1\n",
       "3        1\n",
       "9        1\n",
       "11       1"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a dataframe with predictions\n",
    "predictions_df = pd.DataFrame(predictions, index=test_df.index, columns=['target']).astype('int64')\n",
    "predictions_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save predictions for submission to Kaggle\n",
    "\n",
    "Kaggle expects a 2-column CSV file. First column containing the _id_ and the second one our predicted _target_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to a .csv file\n",
    "predictions_df.to_csv(r'data/predictions.csv', header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A file with predictions has been saved in \"data/predictions.csv\". Now we can upload it to Kaggle manually and find out how it performed."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

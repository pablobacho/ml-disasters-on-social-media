{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch and bag-of-words\n",
    "\n",
    "Here I use the bag-of-words method and a PyTorch neural network to build the model.\n",
    "\n",
    "## Set up environment\n",
    "\n",
    "### Download dataset manually\n",
    "\n",
    "Let's start downloading the train dataset from Kaggle's website:\n",
    "\n",
    "https://www.kaggle.com/c/nlp-getting-started/data\n",
    "\n",
    "I downloaded the dataset manually and saved if in the 'input' folder.\n",
    "\n",
    "The dataset could be downloaded programmatically using Kaggle API, but it requires authentication and it would be unsafe on a notebook that is meant to be shared."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read dataset from CSV\n",
    "\n",
    "Read the dataset from the CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   keyword location                                               text\n",
       "id                                                                    \n",
       "1      NaN      NaN  Our Deeds are the Reason of this #earthquake M...\n",
       "4      NaN      NaN             Forest fire near La Ronge Sask. Canada\n",
       "5      NaN      NaN  All residents asked to 'shelter in place' are ...\n",
       "6      NaN      NaN  13,000 people receive #wildfires evacuation or...\n",
       "7      NaN      NaN  Just got sent this photo from Ruby #Alaska as ..."
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Read train dataset\n",
    "train_df = pd.read_csv('input/train.csv', encoding = 'ISO-8859-1', index_col='id')\n",
    "train_labels = train_df.pop('target').tolist()\n",
    "\n",
    "# Read test dataset\n",
    "test_df = pd.read_csv('input/test.csv', encoding = 'ISO-8859-1', index_col='id')\n",
    "\n",
    "# Check\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text tokenization\n",
    "\n",
    "Tweets are tokenized in a multi-step process:\n",
    "1. **Remove hyperlinks:** Replace hyperlinks with the keyword “islink”.\n",
    "1. **Numbers:** Replace all numbers with the keyword “isnumber”.\n",
    "1. **Punctuation:** Remove any non-alphanumeric character.\n",
    "1. **Case:** Make all tweets lowercase.\n",
    "1. **Split** text into different words.\n",
    "1. **Remove stopwords**.\n",
    "1. **Stem words**.\n",
    "\n",
    "I use the _Natural Language Toolkit_ and _Regular Expressions_ to tokenize tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import *\n",
    "import re\n",
    "\n",
    "def tweet_to_words(text):\n",
    "    nltk.download('stopwords', quiet=True)\n",
    "    stemmer = PorterStemmer()\n",
    "\n",
    "    text = re.sub(r'http\\S+', ' islink ', text) # replace hyperlinks with keyword\n",
    "    text = re.sub(r'(?!,$)[\\d,.]+', ' isnumber ', text) # replace numbers with keyword\n",
    "    text = re.sub(r'[^a-zA-Z0-0]', ' ', text.lower()) # Remove non-alphanumeric characters and convert to lower case\n",
    "    words = text.split() # Split string into words\n",
    "    words = [w for w in words if w not in stopwords.words(\"english\")] # Remove stopwords\n",
    "    words = [PorterStemmer().stem(w) for w in words] # stem\n",
    "    \n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>words</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>[deed, reason, earthquak, may, allah, forgiv, us]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>[forest, fire, near, la, rong, sask, isnumb, c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>[resid, ask, shelter, place, notifi, offic, is...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>[isnumb, peopl, receiv, wildfir, evacu, order,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>[got, sent, photo, rubi, alaska, smoke, wildfi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   keyword location                                               text  \\\n",
       "id                                                                       \n",
       "1      NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "4      NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "5      NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "6      NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "7      NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "                                                words  \n",
       "id                                                     \n",
       "1   [deed, reason, earthquak, may, allah, forgiv, us]  \n",
       "4   [forest, fire, near, la, rong, sask, isnumb, c...  \n",
       "5   [resid, ask, shelter, place, notifi, offic, is...  \n",
       "6   [isnumb, peopl, receiv, wildfir, evacu, order,...  \n",
       "7   [got, sent, photo, rubi, alaska, smoke, wildfi...  "
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train dataset\n",
    "words = []\n",
    "for index, row in train_df.iterrows():\n",
    "    words.append(tweet_to_words(row['text']))\n",
    "\n",
    "train_df['words'] = words\n",
    "\n",
    "# Test dataset\n",
    "words = []\n",
    "for index, row in test_df.iterrows():\n",
    "    words.append(tweet_to_words(row['text']))\n",
    "\n",
    "test_df['words'] = words\n",
    "\n",
    "words = [] # Free words\n",
    "\n",
    "train_df.head() # Check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build vocabulary\n",
    "\n",
    "Create a vocabulary using the tokens found in the train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create word dictionary\n",
    "def build_vocabulary(data, vocabulary_size = 5000):\n",
    "    \"\"\"Construct and return a dictionary mapping each of the most frequently appearing words to a unique integer.\"\"\"\n",
    "    \n",
    "    # TODO: Determine how often each word appears in `data`. Note that `data` is a list of sentences and that a\n",
    "    #       sentence is a list of words.\n",
    "    \n",
    "    word_count = {} # A dict storing the words that appear in the reviews along with how often they occur\n",
    "    for sentence in data:\n",
    "        for word in sentence:\n",
    "            if word in word_count:\n",
    "                word_count[word] += 1\n",
    "            else:\n",
    "                word_count[word] = 1\n",
    "                \n",
    "    # TODO: Sort the words found in `data` so that sorted_words[0] is the most frequently appearing word and\n",
    "    #       sorted_words[-1] is the least frequently appearing word.\n",
    "    \n",
    "    import operator\n",
    "    \n",
    "    sorted_words = None\n",
    "    sorted_words = [item[0] for item in sorted(word_count.items(), key=lambda x: x[1], reverse=True)]\n",
    "        \n",
    "    word_dict = {} # This is what we are building, a dictionary that translates words into integers\n",
    "    for idx, word in enumerate(sorted_words[:vocabulary_size - 2]): # The -2 is so that we save room for the 'no word'\n",
    "        word_dict[word] = idx + 2                              # 'infrequent' labels\n",
    "        \n",
    "    return word_dict, word_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 3058\n",
      "8096 isnumb\n",
      "4721 islink\n",
      "411 like\n",
      "363 fire\n",
      "344 amp\n",
      "311 get\n",
      "239 bomb\n",
      "228 new\n",
      "220 via\n",
      "213 news\n",
      "209 one\n",
      "204 go\n"
     ]
    }
   ],
   "source": [
    "# Build dictionary\n",
    "vocabulary_size = 3000\n",
    "word_vocabulary, word_count = build_vocabulary(train_df['words'], vocabulary_size=vocabulary_size)\n",
    "\n",
    "# dict size\n",
    "print(\"Vocabulary size:\", len(word_vocabulary))\n",
    "\n",
    "# Ten most frequently appearing words in the training set.\n",
    "n = 0\n",
    "for word in word_vocabulary:\n",
    "    print(word_count[word], word)\n",
    "    if n > 10:\n",
    "        break\n",
    "    n += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's find out a little bit more about the vocabulary distribution and make sure we picked the right vocabulary size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of words found: 13066\n",
      "Number of words found 4 times or more: 2956\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Count how many different words are found in tweets\n",
    "word_count_list = []\n",
    "for key, value in word_count.items():\n",
    "    word_count_list.append(value)\n",
    "print('Total number of words found:', len(word_count_list))\n",
    "\n",
    "# Plot the distribution of words\n",
    "plt.plot(sorted(word_count_list, reverse=True)[2:vocabulary_size])\n",
    "\n",
    "# Find whow many words are found N times or more\n",
    "word_threshold = 4\n",
    "word_count_list = [word_count for word_count in word_count_list if word_count >= word_threshold]\n",
    "\n",
    "print('Number of words found', str(word_threshold), 'times or more:', len(word_count_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Codification of tokens\n",
    "\n",
    "Turn tweets into an array of numbers where each number is a word in the vocabulary. Also, the algorithm needs all tweets to be the same length, so we need to choose a tweet length and pad those that are shorter with \"NOWORD\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_and_pad(word_dict, sentence, pad=15):\n",
    "    NOWORD = 0 # We will use 0 to represent the 'no word' category\n",
    "    INFREQ = 1 # and we use 1 to represent the infrequent words, i.e., words not appearing in word_dict\n",
    "    \n",
    "    working_sentence = [NOWORD] * pad\n",
    "    \n",
    "    for word_index, word in enumerate(sentence[:pad]):\n",
    "        if word in word_dict:\n",
    "            working_sentence[word_index] = word_dict[word]\n",
    "        else:\n",
    "            working_sentence[word_index] = INFREQ\n",
    "            \n",
    "    return working_sentence, min(len(sentence), pad)\n",
    "\n",
    "def convert_and_pad_data(word_dict, data, pad=15):\n",
    "    result = []\n",
    "    lengths = []\n",
    "    \n",
    "    for sentence in data:\n",
    "        converted, leng = convert_and_pad(word_dict, sentence, pad)\n",
    "        result.append(converted)\n",
    "        lengths.append(leng)\n",
    "        \n",
    "    return np.array(result), np.array(lengths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose the right length\n",
    "\n",
    "In order to save some RAM, I am not looking to cover the length of the longest tweet, but the vast majority."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To cover 99.9 % of tweets the right length is: 26\n",
      "The longest tweet is 32 words long\n"
     ]
    }
   ],
   "source": [
    "lengths = []\n",
    "for sentence in train_df['words']:\n",
    "    lengths.append(len(sentence))\n",
    "    \n",
    "percentile = 99.9\n",
    "padded_length = int(np.percentile(lengths, percentile))\n",
    "print('To cover', percentile, '% of tweets the right length is:', padded_length)\n",
    "print('The longest tweet is', len(max(train_df['words'], key=len)), 'words long')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Codification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, train_x_len = convert_and_pad_data(word_vocabulary, train_df['words'], pad=padded_length)\n",
    "test_x, test_x_len = convert_and_pad_data(word_vocabulary, test_df['words'], pad=padded_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that conversion went well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index generated randomly: 530\n",
      "Original text: I saw two great punk bands making original music last week. Check em out here!! @GHOSTOFTHEAV @MontroseBand https://t.co/WdvxjsQwic\n",
      "Tokenized: ['saw', 'two', 'great', 'punk', 'band', 'make', 'origin', 'music', 'last', 'week', 'isnumb', 'check', 'em', 'ghostoftheav', 'montroseband', 'islink']\n",
      "Codified: [ 397   67  169    1 1746   36 1094  495  100  271    2  227  786    1\n",
      "    1    3    0    0    0    0    0    0    0    0    0    0]\n"
     ]
    }
   ],
   "source": [
    "idx = np.random.randint(len(train_df))\n",
    "print('Index generated randomly:', idx)\n",
    "print('Original text:', train_df.iloc[idx]['text'])\n",
    "print('Tokenized:', train_df.iloc[idx]['words'])\n",
    "print('Codified:', train_x[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save data to train the model\n",
    "\n",
    "### Save vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "data_dir = 'data' # The folder we will use for storing data\n",
    "if not os.path.exists(data_dir): # Make sure that the folder exists\n",
    "    os.makedirs(data_dir)\n",
    "\n",
    "with open(os.path.join(data_dir, 'word_vocab.pkl'), \"wb\") as f:\n",
    "    pickle.dump(word_vocabulary, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save training dataset\n",
    "\n",
    "Save the post-processed training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([pd.DataFrame(train_labels), pd.DataFrame(train_x_len), pd.DataFrame(train_x)], axis=1).to_csv(os.path.join(data_dir, 'train.csv'), header=False, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload data so S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload data to S3\n",
    "import sagemaker\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "prefix = 'sagemaker/cp01'\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "input_data = sagemaker_session.upload_data(path=data_dir, bucket=bucket, key_prefix=prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the model\n",
    "\n",
    "from sagemaker.pytorch import PyTorch\n",
    "\n",
    "estimator = PyTorch(entry_point=\"train.py\",\n",
    "                    source_dir=\"train\",\n",
    "                    role=role,\n",
    "                    framework_version='0.4.0',\n",
    "                    train_instance_count=1,\n",
    "                    train_instance_type='ml.m4.xlarge',\n",
    "                    hyperparameters={\n",
    "                        'epochs': 10,\n",
    "                        'hidden_dim': 200,\n",
    "                        'vocab_size': vocabulary_size\n",
    "                    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-01-19 20:47:14 Starting - Starting the training job...\n",
      "2020-01-19 20:47:16 Starting - Launching requested ML instances......\n",
      "2020-01-19 20:48:17 Starting - Preparing the instances for training...\n",
      "2020-01-19 20:48:59 Downloading - Downloading input data...\n",
      "2020-01-19 20:49:43 Training - Training image download completed. Training in progress..\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2020-01-19 20:49:44,373 sagemaker-containers INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2020-01-19 20:49:44,375 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2020-01-19 20:49:44,388 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2020-01-19 20:49:44,392 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2020-01-19 20:49:44,615 sagemaker-containers INFO     Module train does not provide a setup.py. \u001b[0m\n",
      "\u001b[34mGenerating setup.py\u001b[0m\n",
      "\u001b[34m2020-01-19 20:49:44,615 sagemaker-containers INFO     Generating setup.cfg\u001b[0m\n",
      "\u001b[34m2020-01-19 20:49:44,615 sagemaker-containers INFO     Generating MANIFEST.in\u001b[0m\n",
      "\u001b[34m2020-01-19 20:49:44,616 sagemaker-containers INFO     Installing module with the following command:\u001b[0m\n",
      "\u001b[34m/usr/bin/python -m pip install -U . -r requirements.txt\u001b[0m\n",
      "\u001b[34mProcessing /opt/ml/code\u001b[0m\n",
      "\u001b[34mCollecting pandas (from -r requirements.txt (line 1))\n",
      "  Downloading https://files.pythonhosted.org/packages/74/24/0cdbf8907e1e3bc5a8da03345c23cbed7044330bb8f73bb12e711a640a00/pandas-0.24.2-cp35-cp35m-manylinux1_x86_64.whl (10.0MB)\u001b[0m\n",
      "\u001b[34mCollecting numpy (from -r requirements.txt (line 2))\n",
      "  Downloading https://files.pythonhosted.org/packages/52/e6/1715e592ef47f28f3f50065322423bb75619ed2f7c24be86380ecc93503c/numpy-1.18.1-cp35-cp35m-manylinux1_x86_64.whl (19.9MB)\u001b[0m\n",
      "\u001b[34mCollecting nltk (from -r requirements.txt (line 3))\n",
      "  Downloading https://files.pythonhosted.org/packages/f6/1d/d925cfb4f324ede997f6d47bea4d9babba51b49e87a767c170b77005889d/nltk-3.4.5.zip (1.5MB)\u001b[0m\n",
      "\u001b[34mCollecting html5lib (from -r requirements.txt (line 4))\n",
      "  Downloading https://files.pythonhosted.org/packages/a5/62/bbd2be0e7943ec8504b517e62bab011b4946e1258842bc159e5dfde15b96/html5lib-1.0.1-py2.py3-none-any.whl (117kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied, skipping upgrade: python-dateutil>=2.5.0 in /usr/local/lib/python3.5/dist-packages (from pandas->-r requirements.txt (line 1)) (2.7.5)\u001b[0m\n",
      "\u001b[34mCollecting pytz>=2011k (from pandas->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34m  Downloading https://files.pythonhosted.org/packages/e7/f9/f0b53f88060247251bf481fa6ea62cd0d25bf1b11a87888e53ce5b7c8ad2/pytz-2019.3-py2.py3-none-any.whl (509kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.5/dist-packages (from nltk->-r requirements.txt (line 3)) (1.11.0)\u001b[0m\n",
      "\u001b[34mCollecting webencodings (from html5lib->-r requirements.txt (line 4))\n",
      "  Downloading https://files.pythonhosted.org/packages/f4/24/2a3e3df732393fed8b3ebf2ec078f05546de641fe1b667ee316ec1dcf3b7/webencodings-0.5.1-py2.py3-none-any.whl\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: nltk, train\n",
      "  Running setup.py bdist_wheel for nltk: started\u001b[0m\n",
      "\u001b[34m  Running setup.py bdist_wheel for nltk: finished with status 'done'\n",
      "  Stored in directory: /root/.cache/pip/wheels/96/86/f6/68ab24c23f207c0077381a5e3904b2815136b879538a24b483\n",
      "  Running setup.py bdist_wheel for train: started\n",
      "  Running setup.py bdist_wheel for train: finished with status 'done'\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-pp3e8oea/wheels/35/24/16/37574d11bf9bde50616c67372a334f94fa8356bc7164af8ca3\u001b[0m\n",
      "\u001b[34mSuccessfully built nltk train\u001b[0m\n",
      "\u001b[34mInstalling collected packages: pytz, numpy, pandas, nltk, webencodings, html5lib, train\n",
      "  Found existing installation: numpy 1.15.4\n",
      "    Uninstalling numpy-1.15.4:\u001b[0m\n",
      "\u001b[34m      Successfully uninstalled numpy-1.15.4\u001b[0m\n",
      "\u001b[34mSuccessfully installed html5lib-1.0.1 nltk-3.4.5 numpy-1.18.1 pandas-0.24.2 pytz-2019.3 train-1.0.0 webencodings-0.5.1\u001b[0m\n",
      "\u001b[34mYou are using pip version 18.1, however version 19.3.1 is available.\u001b[0m\n",
      "\u001b[34mYou should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[34m2020-01-19 20:49:56,088 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2020-01-19 20:49:56,101 sagemaker-containers INFO     Invoking user script\n",
      "\u001b[0m\n",
      "\u001b[34mTraining Env:\n",
      "\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"hyperparameters\": {\n",
      "        \"epochs\": 10,\n",
      "        \"hidden_dim\": 200\n",
      "    },\n",
      "    \"channel_input_dirs\": {\n",
      "        \"training\": \"/opt/ml/input/data/training\"\n",
      "    },\n",
      "    \"module_dir\": \"s3://sagemaker-eu-west-1-496791827306/sagemaker-pytorch-2020-01-19-20-47-14-268/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train\",\n",
      "    \"log_level\": 20,\n",
      "    \"num_gpus\": 0,\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"num_cpus\": 4,\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"network_interface_name\": \"eth0\",\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ]\n",
      "    },\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"input_data_config\": {\n",
      "        \"training\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"job_name\": \"sagemaker-pytorch-2020-01-19-20-47-14-268\",\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"user_entry_point\": \"train.py\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ]\u001b[0m\n",
      "\u001b[34m}\n",
      "\u001b[0m\n",
      "\u001b[34mEnvironment variables:\n",
      "\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"training\"]\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=train.py\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-eu-west-1-496791827306/sagemaker-pytorch-2020-01-19-20-47-14-268/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/usr/local/bin:/usr/lib/python35.zip:/usr/lib/python3.5:/usr/lib/python3.5/plat-x86_64-linux-gnu:/usr/lib/python3.5/lib-dynload:/usr/local/lib/python3.5/dist-packages:/usr/lib/python3/dist-packages\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_HPS={\"epochs\":10,\"hidden_dim\":200}\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAINING=/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_HP_HIDDEN_DIM=200\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"epochs\":10,\"hidden_dim\":200},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"job_name\":\"sagemaker-pytorch-2020-01-19-20-47-14-268\",\"log_level\":20,\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-eu-west-1-496791827306/sagemaker-pytorch-2020-01-19-20-47-14-268/source/sourcedir.tar.gz\",\"module_name\":\"train\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train.py\"}\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=4\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--epochs\",\"10\",\"--hidden_dim\",\"200\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=train\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCHS=10\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=0\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\n",
      "\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\n",
      "\u001b[0m\n",
      "\u001b[34m/usr/bin/python -m train --epochs 10 --hidden_dim 200\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[34mUsing device cpu.\u001b[0m\n",
      "\u001b[34mGet train data loader.\u001b[0m\n",
      "\u001b[34mModel loaded with embedding_dim 32, hidden_dim 200, vocab_size 5000.\u001b[0m\n",
      "\u001b[34mEpoch: 1, BCELoss: 0.6758894801139832\u001b[0m\n",
      "\u001b[34mEpoch: 2, BCELoss: 0.6555433352788289\u001b[0m\n",
      "\u001b[34mEpoch: 3, BCELoss: 0.6449342648188273\u001b[0m\n",
      "\u001b[34mEpoch: 4, BCELoss: 0.6246638655662536\u001b[0m\n",
      "\u001b[34mEpoch: 5, BCELoss: 0.6067038297653198\u001b[0m\n",
      "\u001b[34mEpoch: 6, BCELoss: 0.5876511931419373\u001b[0m\n",
      "\u001b[34mEpoch: 7, BCELoss: 0.5670337478319804\u001b[0m\n",
      "\u001b[34mEpoch: 8, BCELoss: 0.5459219594796498\u001b[0m\n",
      "\u001b[34mEpoch: 9, BCELoss: 0.5220770120620728\u001b[0m\n",
      "\n",
      "2020-01-19 20:51:12 Uploading - Uploading generated training model\n",
      "2020-01-19 20:51:12 Completed - Training job completed\n",
      "\u001b[34mEpoch: 10, BCELoss: 0.49679131706555685\u001b[0m\n",
      "\u001b[34m2020-01-19 20:51:01,045 sagemaker-containers INFO     Reporting training SUCCESS\u001b[0m\n",
      "Training seconds: 133\n",
      "Billable seconds: 133\n"
     ]
    }
   ],
   "source": [
    "estimator.fit({'training': input_data})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------!"
     ]
    }
   ],
   "source": [
    "# Deploy estimator\n",
    "\n",
    "# Remember to shut down the endpoint when we are done!!!!\n",
    "\n",
    "predictor = estimator.deploy(initial_instance_count=1, instance_type='ml.m4.xlarge')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing\n",
    "\n",
    "### Prepare test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>...</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>203</td>\n",
       "      <td>1711</td>\n",
       "      <td>57</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9</td>\n",
       "      <td>418</td>\n",
       "      <td>224</td>\n",
       "      <td>855</td>\n",
       "      <td>140</td>\n",
       "      <td>2</td>\n",
       "      <td>422</td>\n",
       "      <td>1277</td>\n",
       "      <td>194</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12</td>\n",
       "      <td>148</td>\n",
       "      <td>5</td>\n",
       "      <td>612</td>\n",
       "      <td>2944</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>742</td>\n",
       "      <td>471</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>396</td>\n",
       "      <td>277</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>110</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>191</td>\n",
       "      <td>593</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>309</td>\n",
       "      <td>1048</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   0    0     1    2     3    4     5     6    7    8   ...  16  17  18  19  \\\n",
       "0   4  203  1711   57    21    0     0     0    0    0  ...   0   0   0   0   \n",
       "1   9  418   224  855   140    2   422  1277  194    2  ...   0   0   0   0   \n",
       "2  12  148     5  612  2944    2     1     1  742  471  ...   0   0   0   0   \n",
       "3   5  396   277    2     1  110     0     0    0    0  ...   0   0   0   0   \n",
       "4   6  191   593   16     2  309  1048     0    0    0  ...   0   0   0   0   \n",
       "\n",
       "   20  21  22  23  24  25  \n",
       "0   0   0   0   0   0   0  \n",
       "1   0   0   0   0   0   0  \n",
       "2   0   0   0   0   0   0  \n",
       "3   0   0   0   0   0   0  \n",
       "4   0   0   0   0   0   0  \n",
       "\n",
       "[5 rows x 27 columns]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_X = pd.concat([pd.DataFrame(test_x_len), pd.DataFrame(test_x)], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We split the data into chunks and send each chunk seperately, accumulating the results.\n",
    "def predict(data, rows=512):\n",
    "    split_array = np.array_split(data, int(data.shape[0] / float(rows) + 1))\n",
    "    predictions = np.array([])\n",
    "    for array in split_array:\n",
    "        predictions = np.append(predictions, predictor.predict(array))\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = predict(test_X.values)\n",
    "rounded_predictions = [round(num) for num in predictions]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save predictions for submission to Kaggle\n",
    "\n",
    "Kaggle expects a 2-column CSV file. First column containing the _id_ and the second one our predicted _target_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    target\n",
       "id        \n",
       "0        1\n",
       "2        0\n",
       "3        1\n",
       "9        0\n",
       "11       1"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prepara dataframe\n",
    "predictions_df = pd.DataFrame(rounded_predictions, index=test_df.index, columns=['target']).astype('int64')\n",
    "predictions_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to a .csv file\n",
    "predictions_df.to_csv(r'data/predictions.csv', header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A file with predictions has been saved in \"data/predictions.csv\". Now we can upload it to Kaggle manually and find out how it performed."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch and bag-of-words\n",
    "\n",
    "Thinking that position of words might matter, this is my attempt at solving this problem using 2-grams.\n",
    "\n",
    "## Set up environment\n",
    "\n",
    "### Download dataset manually\n",
    "\n",
    "Let's start downloading the train dataset from Kaggle's website:\n",
    "\n",
    "https://www.kaggle.com/c/nlp-getting-started/data\n",
    "\n",
    "I downloaded the dataset manually and saved if in the 'input' folder.\n",
    "\n",
    "The dataset could be downloaded programmatically using Kaggle API, but it requires authentication and it would be unsafe on a notebook that is meant to be shared."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read dataset from CSV\n",
    "\n",
    "Read the dataset from the CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   keyword location                                               text\n",
       "id                                                                    \n",
       "1      NaN      NaN  Our Deeds are the Reason of this #earthquake M...\n",
       "4      NaN      NaN             Forest fire near La Ronge Sask. Canada\n",
       "5      NaN      NaN  All residents asked to 'shelter in place' are ...\n",
       "6      NaN      NaN  13,000 people receive #wildfires evacuation or...\n",
       "7      NaN      NaN  Just got sent this photo from Ruby #Alaska as ..."
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Read train dataset\n",
    "train_df = pd.read_csv('input/train.csv', encoding = 'ISO-8859-1', index_col='id')\n",
    "train_labels = train_df.pop('target').tolist()\n",
    "\n",
    "# Read test dataset\n",
    "test_df = pd.read_csv('input/test.csv', encoding = 'ISO-8859-1', index_col='id')\n",
    "\n",
    "# Check\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text tokenization\n",
    "\n",
    "Tweets are tokenized in a multi-step process:\n",
    "1. **Remove hyperlinks:** Replace hyperlinks with the keyword “islink”.\n",
    "1. **Numbers:** Replace all numbers with the keyword “isnumber”.\n",
    "1. **Punctuation:** Remove any non-alphanumeric character.\n",
    "1. **Case:** Make all tweets lowercase.\n",
    "1. **Split** text into different words.\n",
    "1. **Remove stopwords**.\n",
    "1. **Stem words**.\n",
    "\n",
    "I use the _Natural Language Toolkit_ and _Regular Expressions_ to tokenize tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import *\n",
    "import re\n",
    "\n",
    "def tweet_to_2grams(text):\n",
    "    nltk.download('stopwords', quiet=True)\n",
    "    stemmer = PorterStemmer()\n",
    "\n",
    "    text = re.sub(r'http\\S+', ' islink ', text) # replace hyperlinks with keyword\n",
    "    text = re.sub(r'(?!,$)[\\d,.]+', ' isnumber ', text) # replace numbers with keyword\n",
    "    text = re.sub(r'[^a-zA-Z0-0]', ' ', text.lower()) # Remove non-alphanumeric characters and convert to lower case\n",
    "    words = text.split() # Split string into words\n",
    "    words = [w for w in words if w not in stopwords.words(\"english\")] # Remove stopwords\n",
    "    words = [PorterStemmer().stem(w) for w in words] # stem\n",
    "    \n",
    "    # Pair words with their previous one (except \"islink\" keyword)\n",
    "    if(len(words) > 1):\n",
    "        ngrams = []\n",
    "        prev_word = words[0]\n",
    "        for word in words[1:]:\n",
    "            if(word != 'islink'):\n",
    "                ngrams.append(prev_word + ' ' + word)\n",
    "                prev_word = word\n",
    "            else:\n",
    "                ngrams.append(word)\n",
    "        words = ngrams\n",
    "        \n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>words</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>[deed reason, reason earthquak, earthquak may,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>[forest fire, fire near, near la, la rong, ron...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>[resid ask, ask shelter, shelter place, place ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>[isnumb peopl, peopl receiv, receiv wildfir, w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>[got sent, sent photo, photo rubi, rubi alaska...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   keyword location                                               text  \\\n",
       "id                                                                       \n",
       "1      NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "4      NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "5      NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "6      NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "7      NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "                                                words  \n",
       "id                                                     \n",
       "1   [deed reason, reason earthquak, earthquak may,...  \n",
       "4   [forest fire, fire near, near la, la rong, ron...  \n",
       "5   [resid ask, ask shelter, shelter place, place ...  \n",
       "6   [isnumb peopl, peopl receiv, receiv wildfir, w...  \n",
       "7   [got sent, sent photo, photo rubi, rubi alaska...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train dataset\n",
    "words = []\n",
    "for index, row in train_df.iterrows():\n",
    "    words.append(tweet_to_2grams(row['text']))\n",
    "\n",
    "train_df['words'] = words\n",
    "\n",
    "# Test dataset\n",
    "words = []\n",
    "for index, row in test_df.iterrows():\n",
    "    words.append(tweet_to_2grams(row['text']))\n",
    "\n",
    "test_df['words'] = words\n",
    "\n",
    "words = [] # Free words\n",
    "\n",
    "train_df.head() # Check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build vocabulary\n",
    "\n",
    "Create a vocabulary using the tokens found in the train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create word dictionary\n",
    "def build_vocabulary(data, vocabulary_size = 5000):\n",
    "    \"\"\"Construct and return a dictionary mapping each of the most frequently appearing words to a unique integer.\"\"\"\n",
    "    \n",
    "    # TODO: Determine how often each word appears in `data`. Note that `data` is a list of sentences and that a\n",
    "    #       sentence is a list of words.\n",
    "    \n",
    "    word_count = {} # A dict storing the words that appear in the reviews along with how often they occur\n",
    "    for sentence in data:\n",
    "        for word in sentence:\n",
    "            if word in word_count:\n",
    "                word_count[word] += 1\n",
    "            else:\n",
    "                word_count[word] = 1\n",
    "                \n",
    "    # TODO: Sort the words found in `data` so that sorted_words[0] is the most frequently appearing word and\n",
    "    #       sorted_words[-1] is the least frequently appearing word.\n",
    "    \n",
    "    import operator\n",
    "    \n",
    "    sorted_words = None\n",
    "    sorted_words = [item[0] for item in sorted(word_count.items(), key=lambda x: x[1], reverse=True)]\n",
    "        \n",
    "    word_dict = {} # This is what we are building, a dictionary that translates words into integers\n",
    "    for idx, word in enumerate(sorted_words[:vocabulary_size - 2]): # The -2 is so that we save room for the 'no word'\n",
    "        word_dict[word] = idx + 2                              # 'infrequent' labels\n",
    "        \n",
    "    return word_dict, word_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 2998\n",
      "4663 islink\n",
      "863 isnumb isnumb\n",
      "95 isnumb year\n",
      "80 bodi bag\n",
      "73 isnumb pm\n",
      "73 mh isnumb\n",
      "64 mass murder\n",
      "60 suicid bomber\n",
      "59 burn build\n",
      "55 look like\n",
      "53 u isnumb\n",
      "52 fire isnumb\n"
     ]
    }
   ],
   "source": [
    "# Build dictionary\n",
    "vocabulary_size = 3000\n",
    "word_vocabulary, word_count = build_vocabulary(train_df['words'], vocabulary_size=vocabulary_size)\n",
    "\n",
    "# dict size\n",
    "print(\"Vocabulary size:\", len(word_vocabulary))\n",
    "\n",
    "# Ten most frequently appearing words in the training set.\n",
    "n = 0\n",
    "for word in word_vocabulary:\n",
    "    print(word_count[word], word)\n",
    "    if n > 10:\n",
    "        break\n",
    "    n += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's find out a little bit more about the vocabulary distribution and make sure we picked the right vocabulary size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of words found: 47374\n",
      "Number of words found 4 times or more: 2111\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Count how many different words are found in tweets\n",
    "word_count_list = []\n",
    "for key, value in word_count.items():\n",
    "    word_count_list.append(value)\n",
    "print('Total number of words found:', len(word_count_list))\n",
    "\n",
    "# Plot the distribution of words\n",
    "plt.plot(sorted(word_count_list, reverse=True)[2:vocabulary_size])\n",
    "\n",
    "# Find whow many words are found N times or more\n",
    "word_threshold = 4\n",
    "word_count_list = [word_count for word_count in word_count_list if word_count >= word_threshold]\n",
    "\n",
    "print('Number of words found', str(word_threshold), 'times or more:', len(word_count_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Codification of tokens\n",
    "\n",
    "Turn tweets into an array of numbers where each number is a word in the vocabulary. Also, the algorithm needs all tweets to be the same length, so we need to choose a tweet length and pad those that are shorter with \"NOWORD\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_and_pad(word_dict, sentence, pad=15):\n",
    "    NOWORD = 0 # We will use 0 to represent the 'no word' category\n",
    "    INFREQ = 1 # and we use 1 to represent the infrequent words, i.e., words not appearing in word_dict\n",
    "    \n",
    "    working_sentence = [NOWORD] * pad\n",
    "    \n",
    "    for word_index, word in enumerate(sentence[:pad]):\n",
    "        if word in word_dict:\n",
    "            working_sentence[word_index] = word_dict[word]\n",
    "        else:\n",
    "            working_sentence[word_index] = INFREQ\n",
    "            \n",
    "    return working_sentence, min(len(sentence), pad)\n",
    "\n",
    "def convert_and_pad_data(word_dict, data, pad=15):\n",
    "    result = []\n",
    "    lengths = []\n",
    "    \n",
    "    for sentence in data:\n",
    "        converted, leng = convert_and_pad(word_dict, sentence, pad)\n",
    "        result.append(converted)\n",
    "        lengths.append(leng)\n",
    "        \n",
    "    return np.array(result), np.array(lengths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose the right length\n",
    "\n",
    "In order to save some RAM, I am not looking to cover the length of the longest tweet, but the vast majority."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To cover 99.9 % of tweets the right length is: 25\n",
      "The longest tweet is 31 words long\n"
     ]
    }
   ],
   "source": [
    "lengths = []\n",
    "for sentence in train_df['words']:\n",
    "    lengths.append(len(sentence))\n",
    "    \n",
    "percentile = 99.9\n",
    "padded_length = int(np.percentile(lengths, percentile))\n",
    "print('To cover', percentile, '% of tweets the right length is:', padded_length)\n",
    "print('The longest tweet is', len(max(train_df['words'], key=len)), 'words long')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Codification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, train_x_len = convert_and_pad_data(word_vocabulary, train_df['words'], pad=padded_length)\n",
    "test_x, test_x_len = convert_and_pad_data(word_vocabulary, test_df['words'], pad=padded_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that conversion went well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index generated randomly: 7137\n",
      "Original text: #USGS M 1.9 - 5km S of Volcano Hawaii: Time2015-08-06 01:04:01 UTC2015-08-05 15:04:01 -10:00 at epicenter... http://t.co/dIsrwhQGym #SM\n",
      "Tokenized: ['usg isnumb', 'isnumb isnumb', 'isnumb km', 'km volcano', 'volcano hawaii', 'hawaii time', 'time isnumb', 'isnumb isnumb', 'isnumb isnumb', 'isnumb isnumb', 'isnumb isnumb', 'isnumb isnumb', 'isnumb utc', 'utc isnumb', 'isnumb isnumb', 'isnumb isnumb', 'isnumb isnumb', 'isnumb isnumb', 'isnumb isnumb', 'isnumb isnumb', 'isnumb isnumb', 'isnumb epicent', 'epicent isnumb', 'islink', 'isnumb sm']\n",
      "Codified: [1849    3   42  508  509 2100   25    3    3    3    3    3   96  197\n",
      "    3    3    3    3    3    3    3    1    1    2 2842]\n"
     ]
    }
   ],
   "source": [
    "idx = np.random.randint(len(train_df))\n",
    "print('Index generated randomly:', idx)\n",
    "print('Original text:', train_df.iloc[idx]['text'])\n",
    "print('Tokenized:', train_df.iloc[idx]['words'])\n",
    "print('Codified:', train_x[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save data to train the model\n",
    "\n",
    "### Save vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "data_dir = 'data' # The folder we will use for storing data\n",
    "if not os.path.exists(data_dir): # Make sure that the folder exists\n",
    "    os.makedirs(data_dir)\n",
    "\n",
    "with open(os.path.join(data_dir, 'word_vocab.pkl'), \"wb\") as f:\n",
    "    pickle.dump(word_vocabulary, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save training dataset\n",
    "\n",
    "Save the post-processed training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([pd.DataFrame(train_labels), pd.DataFrame(train_x_len), pd.DataFrame(train_x)], axis=1).to_csv(os.path.join(data_dir, 'train.csv'), header=False, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload data so S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload data to S3\n",
    "import sagemaker\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "prefix = 'sagemaker/cp01'\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "input_data = sagemaker_session.upload_data(path=data_dir, bucket=bucket, key_prefix=prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the model\n",
    "\n",
    "from sagemaker.pytorch import PyTorch\n",
    "\n",
    "estimator = PyTorch(entry_point=\"train.py\",\n",
    "                    source_dir=\"train\",\n",
    "                    role=role,\n",
    "                    framework_version='0.4.0',\n",
    "                    train_instance_count=1,\n",
    "                    train_instance_type='ml.m4.xlarge',\n",
    "                    hyperparameters={\n",
    "                        'epochs': 10,\n",
    "                        'hidden_dim': 200,\n",
    "                        'vocab_size': vocabulary_size\n",
    "                    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-01-20 15:47:52 Starting - Starting the training job...\n",
      "2020-01-20 15:47:54 Starting - Launching requested ML instances...\n",
      "2020-01-20 15:48:51 Starting - Preparing the instances for training......\n",
      "2020-01-20 15:49:45 Downloading - Downloading input data...\n",
      "2020-01-20 15:50:12 Training - Downloading the training image.\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2020-01-20 15:50:24,867 sagemaker-containers INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2020-01-20 15:50:24,870 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2020-01-20 15:50:24,883 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2020-01-20 15:50:24,886 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2020-01-20 15:50:25,120 sagemaker-containers INFO     Module train does not provide a setup.py. \u001b[0m\n",
      "\u001b[34mGenerating setup.py\u001b[0m\n",
      "\u001b[34m2020-01-20 15:50:25,121 sagemaker-containers INFO     Generating setup.cfg\u001b[0m\n",
      "\u001b[34m2020-01-20 15:50:25,121 sagemaker-containers INFO     Generating MANIFEST.in\u001b[0m\n",
      "\u001b[34m2020-01-20 15:50:25,121 sagemaker-containers INFO     Installing module with the following command:\u001b[0m\n",
      "\u001b[34m/usr/bin/python -m pip install -U . -r requirements.txt\u001b[0m\n",
      "\u001b[34mProcessing /opt/ml/code\u001b[0m\n",
      "\u001b[34mCollecting pandas (from -r requirements.txt (line 1))\n",
      "  Downloading https://files.pythonhosted.org/packages/74/24/0cdbf8907e1e3bc5a8da03345c23cbed7044330bb8f73bb12e711a640a00/pandas-0.24.2-cp35-cp35m-manylinux1_x86_64.whl (10.0MB)\u001b[0m\n",
      "\u001b[34mCollecting numpy (from -r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34m  Downloading https://files.pythonhosted.org/packages/52/e6/1715e592ef47f28f3f50065322423bb75619ed2f7c24be86380ecc93503c/numpy-1.18.1-cp35-cp35m-manylinux1_x86_64.whl (19.9MB)\u001b[0m\n",
      "\u001b[34mCollecting nltk (from -r requirements.txt (line 3))\n",
      "  Downloading https://files.pythonhosted.org/packages/f6/1d/d925cfb4f324ede997f6d47bea4d9babba51b49e87a767c170b77005889d/nltk-3.4.5.zip (1.5MB)\u001b[0m\n",
      "\u001b[34mCollecting html5lib (from -r requirements.txt (line 4))\n",
      "  Downloading https://files.pythonhosted.org/packages/a5/62/bbd2be0e7943ec8504b517e62bab011b4946e1258842bc159e5dfde15b96/html5lib-1.0.1-py2.py3-none-any.whl (117kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied, skipping upgrade: python-dateutil>=2.5.0 in /usr/local/lib/python3.5/dist-packages (from pandas->-r requirements.txt (line 1)) (2.7.5)\u001b[0m\n",
      "\u001b[34mCollecting pytz>=2011k (from pandas->-r requirements.txt (line 1))\n",
      "  Downloading https://files.pythonhosted.org/packages/e7/f9/f0b53f88060247251bf481fa6ea62cd0d25bf1b11a87888e53ce5b7c8ad2/pytz-2019.3-py2.py3-none-any.whl (509kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.5/dist-packages (from nltk->-r requirements.txt (line 3)) (1.11.0)\u001b[0m\n",
      "\u001b[34mCollecting webencodings (from html5lib->-r requirements.txt (line 4))\n",
      "  Downloading https://files.pythonhosted.org/packages/f4/24/2a3e3df732393fed8b3ebf2ec078f05546de641fe1b667ee316ec1dcf3b7/webencodings-0.5.1-py2.py3-none-any.whl\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: nltk, train\n",
      "  Running setup.py bdist_wheel for nltk: started\u001b[0m\n",
      "\u001b[34m  Running setup.py bdist_wheel for nltk: finished with status 'done'\n",
      "  Stored in directory: /root/.cache/pip/wheels/96/86/f6/68ab24c23f207c0077381a5e3904b2815136b879538a24b483\n",
      "  Running setup.py bdist_wheel for train: started\n",
      "  Running setup.py bdist_wheel for train: finished with status 'done'\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-o4b0alfo/wheels/35/24/16/37574d11bf9bde50616c67372a334f94fa8356bc7164af8ca3\u001b[0m\n",
      "\u001b[34mSuccessfully built nltk train\u001b[0m\n",
      "\u001b[34mInstalling collected packages: numpy, pytz, pandas, nltk, webencodings, html5lib, train\n",
      "  Found existing installation: numpy 1.15.4\n",
      "    Uninstalling numpy-1.15.4:\n",
      "      Successfully uninstalled numpy-1.15.4\u001b[0m\n",
      "\u001b[34mSuccessfully installed html5lib-1.0.1 nltk-3.4.5 numpy-1.18.1 pandas-0.24.2 pytz-2019.3 train-1.0.0 webencodings-0.5.1\u001b[0m\n",
      "\u001b[34mYou are using pip version 18.1, however version 19.3.1 is available.\u001b[0m\n",
      "\u001b[34mYou should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[34m2020-01-20 15:50:36,605 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2020-01-20 15:50:36,619 sagemaker-containers INFO     Invoking user script\n",
      "\u001b[0m\n",
      "\u001b[34mTraining Env:\n",
      "\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"job_name\": \"sagemaker-pytorch-2020-01-20-15-47-52-178\",\n",
      "    \"module_name\": \"train\",\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"module_dir\": \"s3://sagemaker-eu-west-1-496791827306/sagemaker-pytorch-2020-01-20-15-47-52-178/source/sourcedir.tar.gz\",\n",
      "    \"user_entry_point\": \"train.py\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"resource_config\": {\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"log_level\": 20,\n",
      "    \"num_gpus\": 0,\n",
      "    \"channel_input_dirs\": {\n",
      "        \"training\": \"/opt/ml/input/data/training\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"num_cpus\": 4,\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"input_data_config\": {\n",
      "        \"training\": {\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hyperparameters\": {\n",
      "        \"hidden_dim\": 200,\n",
      "        \"epochs\": 10,\n",
      "        \"vocab_size\": 3000\n",
      "    },\n",
      "    \"additional_framework_parameters\": {}\u001b[0m\n",
      "\u001b[34m}\n",
      "\u001b[0m\n",
      "\u001b[34mEnvironment variables:\n",
      "\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/usr/local/bin:/usr/lib/python35.zip:/usr/lib/python3.5:/usr/lib/python3.5/plat-x86_64-linux-gnu:/usr/lib/python3.5/lib-dynload:/usr/local/lib/python3.5/dist-packages:/usr/lib/python3/dist-packages\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"epochs\":10,\"hidden_dim\":200,\"vocab_size\":3000},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"job_name\":\"sagemaker-pytorch-2020-01-20-15-47-52-178\",\"log_level\":20,\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-eu-west-1-496791827306/sagemaker-pytorch-2020-01-20-15-47-52-178/source/sourcedir.tar.gz\",\"module_name\":\"train\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train.py\"}\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=0\u001b[0m\n",
      "\u001b[34mSM_HP_VOCAB_SIZE=3000\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-eu-west-1-496791827306/sagemaker-pytorch-2020-01-20-15-47-52-178/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_HPS={\"epochs\":10,\"hidden_dim\":200,\"vocab_size\":3000}\u001b[0m\n",
      "\u001b[34mSM_HP_HIDDEN_DIM=200\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--epochs\",\"10\",\"--hidden_dim\",\"200\",\"--vocab_size\",\"3000\"]\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAINING=/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCHS=10\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=4\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=train\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"training\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=train.py\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\n",
      "\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\n",
      "\u001b[0m\n",
      "\u001b[34m/usr/bin/python -m train --epochs 10 --hidden_dim 200 --vocab_size 3000\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[34mUsing device cpu.\u001b[0m\n",
      "\u001b[34mGet train data loader.\u001b[0m\n",
      "\u001b[34mModel loaded with embedding_dim 32, hidden_dim 200, vocab_size 3000.\u001b[0m\n",
      "\n",
      "2020-01-20 15:50:25 Training - Training image download completed. Training in progress.\u001b[34mEpoch: 1, BCELoss: 0.6691734115282695\u001b[0m\n",
      "\u001b[34mEpoch: 2, BCELoss: 0.6501954515775045\u001b[0m\n",
      "\u001b[34mEpoch: 3, BCELoss: 0.6465737620989481\u001b[0m\n",
      "\u001b[34mEpoch: 4, BCELoss: 0.6376142144203186\u001b[0m\n",
      "\u001b[34mEpoch: 5, BCELoss: 0.6296037673950196\u001b[0m\n",
      "\u001b[34mEpoch: 6, BCELoss: 0.6206441084543864\u001b[0m\n",
      "\u001b[34mEpoch: 7, BCELoss: 0.6140076875686645\u001b[0m\n",
      "\u001b[34mEpoch: 8, BCELoss: 0.6014522552490235\u001b[0m\n",
      "\u001b[34mEpoch: 9, BCELoss: 0.5898945450782775\u001b[0m\n",
      "\u001b[34mEpoch: 10, BCELoss: 0.582153594493866\u001b[0m\n",
      "\u001b[34m2020-01-20 15:51:38,446 sagemaker-containers INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2020-01-20 15:51:48 Uploading - Uploading generated training model\n",
      "2020-01-20 15:51:48 Completed - Training job completed\n",
      "Training seconds: 123\n",
      "Billable seconds: 123\n"
     ]
    }
   ],
   "source": [
    "estimator.fit({'training': input_data})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------!"
     ]
    }
   ],
   "source": [
    "# Deploy estimator\n",
    "\n",
    "# Remember to shut down the endpoint when we are done!!!!\n",
    "\n",
    "predictor = estimator.deploy(initial_instance_count=1, instance_type='ml.m4.xlarge')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing\n",
    "\n",
    "### Prepare test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_X = pd.concat([pd.DataFrame(test_x_len), pd.DataFrame(test_x)], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We split the data into chunks and send each chunk seperately, accumulating the results.\n",
    "def predict(data, rows=512):\n",
    "    split_array = np.array_split(data, int(data.shape[0] / float(rows) + 1))\n",
    "    predictions = np.array([])\n",
    "    for array in split_array:\n",
    "        predictions = np.append(predictions, predictor.predict(array))\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = predict(test_X.values)\n",
    "rounded_predictions = [round(num) for num in predictions]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save predictions for submission to Kaggle\n",
    "\n",
    "Kaggle expects a 2-column CSV file. First column containing the _id_ and the second one our predicted _target_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    target\n",
       "id        \n",
       "0        0\n",
       "2        1\n",
       "3        0\n",
       "9        0\n",
       "11       0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prepara dataframe\n",
    "predictions_df = pd.DataFrame(rounded_predictions, index=test_df.index, columns=['target']).astype('int64')\n",
    "predictions_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to a .csv file\n",
    "predictions_df.to_csv(r'data/predictions.csv', header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A file with predictions has been saved in \"data/predictions.csv\". Now we can upload it to Kaggle manually and find out how it performed."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
